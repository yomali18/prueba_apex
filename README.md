# prueba_apex

Pipeline ETL desarrollado en PySpark para el procesamiento de datos de entregas de productos.  El flujo implementa buenas prÃ¡cticas de ingenierÃ­a de datos: estandarizaciÃ³n de columnas, control de calidad, filtrado parametrizado, transformaciÃ³n de unidades y generaciÃ³n de mÃ©tricas.

---

## ğŸ“Œ Requisitos

Antes de ejecutar el pipeline es necesario contar con:

- Python 3.9 o superior
- Apache Spark instalado y configurado (`spark-submit` disponible en PATH)
- Java 11 o 17
- Git

Instalar dependencias de Python:

```bash
pip install -r requirements.txt
```
---

## ğŸ—‚ï¸ Estructura 
El proyecto estÃ¡ organizado bajo un enfoque modular para facilitar mantenibilidad, escalabilidad y separaciÃ³n de responsabilidades dentro del flujo ETL. La siguiente estructura refleja los distintos componentes del pipeline, desde la configuraciÃ³n y validaciÃ³n de datos hasta la generaciÃ³n de salidas particionadas.

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_etl.py
â”œâ”€â”€ log4j2.properties
â”œâ”€â”€ config
â”‚   â””â”€â”€ base.yaml
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw
â”‚   â”‚   â””â”€â”€ product_deliveries.csv
â”‚   â””â”€â”€ processed
â”‚       â”œâ”€â”€ pais=PE
â”‚       â”‚   â””â”€â”€ fecha_proceso=20250114
â”‚       â”‚       â””â”€â”€ part-00000.csv
â”‚       â””â”€â”€ pais=GT
â”‚           â””â”€â”€ fecha_proceso=20250513
â”‚               â””â”€â”€ part-00000.csv
â”œâ”€â”€ docs
â”‚   â””â”€â”€ enunciado_prueba.pdf
â”œâ”€â”€ logs
â””â”€â”€ src
    â”œâ”€â”€ main.py
    â”œâ”€â”€ config
    â”œâ”€â”€ filters
    â”‚   â””â”€â”€ selection.py
    â”œâ”€â”€ io
    â”‚   â””â”€â”€ writer.py
    â”œâ”€â”€ transformers
    â”‚   â”œâ”€â”€ units.py
    â”‚   â”œâ”€â”€ deliveries.py
    â”‚   â””â”€â”€ enrichment.py
    â”œâ”€â”€ utils
    â”‚   â”œâ”€â”€ initial_explore.py
    â”‚   â”œâ”€â”€ reporting.py
    â”‚   â””â”€â”€ snake_case.py
    â””â”€â”€ validators
        â””â”€â”€ quality.py

