# prueba_apex

Pipeline ETL desarrollado en PySpark para el procesamiento de datos de entregas de productos.  El flujo implementa buenas prÃ¡cticas de ingenierÃ­a de datos: estandarizaciÃ³n de columnas, control de calidad, filtrado parametrizado, transformaciÃ³n de unidades y generaciÃ³n de mÃ©tricas.

---

## ğŸ“Œ Objetivo

Procesar un dataset de entregas de productos aplicando:

- Validaciones de calidad de datos
- Filtros dinÃ¡micos por rango de fechas y paÃ­s
- NormalizaciÃ³n de unidades
- ClasificaciÃ³n de tipos de entrega
- Enriquecimiento de mÃ©tricas
- ExportaciÃ³n particionada

---

## ğŸ—‚ï¸ Estructura del proyecto
```text
.
â”œâ”€â”€ config
â”‚   â””â”€â”€ base.yaml
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw
â”‚   â”‚   â””â”€â”€ product_deliveries.csv
â”‚   â””â”€â”€ processed
â”‚       â”œâ”€â”€ pais=PE
â”‚       â”‚   â””â”€â”€ fecha_proceso=20250114
â”‚       â””â”€â”€ pais=GT
â”‚           â””â”€â”€ fecha_proceso=20250513
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ transformers
â”‚   â”œâ”€â”€ utils
â”‚   â””â”€â”€ validators
â”œâ”€â”€ run_etl.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
