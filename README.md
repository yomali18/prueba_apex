# prueba_apex

Pipeline ETL desarrollado en PySpark para el procesamiento de datos de entregas de productos.  El flujo implementa buenas prÃ¡cticas de ingenierÃ­a de datos: estandarizaciÃ³n de columnas, control de calidad, filtrado parametrizado, transformaciÃ³n de unidades y generaciÃ³n de mÃ©tricas.

---

## ğŸ“Œ Requisitos

Antes de ejecutar el pipeline es necesario contar con:

- Python 3.9 o superior
- Apache Spark instalado y configurado (`spark-submit` disponible en PATH)
- Java 11 o 17
- Git

Instalar dependencias de Python:

```bash
pip install -r requirements.txt
```
---
## âš™ï¸ ConfiguraciÃ³n

La configuraciÃ³n del pipeline se define en un archivo YAML utilizando OmegaConf (`config/base.yaml`)

```yaml
paths:
  input_csv: data/raw/product_deliveries.csv
  output_base: data/processed

date_range:
  date_column: fecha_proceso
  start_date: 20250101
  end_date: 20250630

filters:
  country: PE
```
---
## â–¶ï¸ Run

El pipeline se ejecuta exclusivamente mediante el archivo `run_etl.py`

```bash
spark-submit run_etl.py \
  --start-date 20250101 \
  --end-date 20250630 \
  --country PE
```
 `country` puede tomar los valores de `EC` (Ecuador), `GT` (Guatemala), `HN` (Honduras), `JM` (Jamaica), `PE` (PerÃº), `SV` (El Salvador)

El rango global de la `fecha_proceso` es `20250114` hasta `20250602`. Por lo tanto, `start-date` y `end-date` deberia estar entre esas fechas.

Para facilitar la revisiÃ³n del flujo y visualizar Ãºnicamente los outputs relevantes del ETL (esquemas, muestras de datos, mÃ©tricas y reportes), el pipeline puede ejecutarse utilizando una configuraciÃ³n de logging reducida.

```bash
spark-submit \
  --conf "spark.driver.extraJavaOptions=-Dlog4j.configurationFile=log4j2.properties" \
  run_etl.py \
  --start-date 20250101 \
  --end-date 20250630 \
  --country PE
```


---
## ğŸ—‚ï¸ Estructura 
El proyecto estÃ¡ organizado bajo un enfoque modular para facilitar mantenibilidad, escalabilidad y separaciÃ³n de responsabilidades dentro del flujo ETL. La siguiente estructura refleja los distintos componentes del pipeline, desde la configuraciÃ³n y validaciÃ³n de datos hasta la generaciÃ³n de salidas particionadas.

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_etl.py
â”œâ”€â”€ log4j2.properties
â”œâ”€â”€ config
â”‚   â””â”€â”€ base.yaml
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw
â”‚   â”‚   â””â”€â”€ product_deliveries.csv
â”‚   â””â”€â”€ processed
â”‚       â”œâ”€â”€ pais=PE
â”‚       â”‚   â””â”€â”€ fecha_proceso=20250114
â”‚       â”‚       â””â”€â”€ part-00000.csv
â”‚       â””â”€â”€ pais=GT
â”‚           â””â”€â”€ fecha_proceso=20250513
â”‚               â””â”€â”€ part-00000.csv
â”œâ”€â”€ docs
â”‚   â””â”€â”€ enunciado_prueba.pdf
â”œâ”€â”€ logs
â””â”€â”€ src
    â”œâ”€â”€ main.py
    â”œâ”€â”€ config
    â”œâ”€â”€ filters
    â”‚   â””â”€â”€ selection.py
    â”œâ”€â”€ io
    â”‚   â””â”€â”€ writer.py
    â”œâ”€â”€ transformers
    â”‚   â”œâ”€â”€ units.py
    â”‚   â”œâ”€â”€ deliveries.py
    â”‚   â””â”€â”€ enrichment.py
    â”œâ”€â”€ utils
    â”‚   â”œâ”€â”€ initial_explore.py
    â”‚   â”œâ”€â”€ reporting.py
    â”‚   â””â”€â”€ snake_case.py
    â””â”€â”€ validators
        â””â”€â”€ quality.py

