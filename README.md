# PRUEBA TECNICA

Pipeline ETL desarrollado en PySpark para el procesamiento de datos de entregas de productos.  El flujo implementa buenas prÃ¡cticas de ingenierÃ­a de datos: estandarizaciÃ³n de columnas, control de calidad, filtrado parametrizado, transformaciÃ³n de unidades y generaciÃ³n de mÃ©tricas.

---

## ðŸ“Œ Requisitos

Antes de ejecutar el pipeline es necesario contar con:

- Python 3.9 o superior
- Apache Spark instalado y configurado (`spark-submit` disponible en PATH)
- Java 11 o 17
- Git

Instalar dependencias de Python:

```bash
pip install -r requirements.txt
```
---
## âš™ï¸ ConfiguraciÃ³n

La configuraciÃ³n del pipeline se define en un archivo YAML utilizando OmegaConf (`config/base.yaml`)

```yaml
paths:
  input_csv: data/raw/product_deliveries.csv
  output_base: data/processed

date_range:
  date_column: fecha_proceso
  start_date: 20250101
  end_date: 20250630

filters:
  country: PE
```
---
## â–¶ï¸ Run

El pipeline se ejecuta exclusivamente mediante el archivo `run_etl.py`

```bash
spark-submit run_etl.py --start-date 20250101 --end-date 20250630 --country PE
```
 `country` puede tomar los valores de `EC` (Ecuador), `GT` (Guatemala), `HN` (Honduras), `JM` (Jamaica), `PE` (PerÃº), `SV` (El Salvador)

El rango global de la `fecha_proceso` es `20250114` hasta `20250602`. Por lo tanto, `start-date` y `end-date` deberia estar entre esas fechas.

Para facilitar la revisiÃ³n del flujo y visualizar Ãºnicamente los outputs relevantes del ETL (esquemas, muestras de datos, mÃ©tricas y reportes), el pipeline puede ejecutarse utilizando una configuraciÃ³n de logging reducida.

```bash
spark-submit --conf "spark.driver.extraJavaOptions=-Dlog4j.configurationFile=log4j2.properties" run_etl.py --start-date 20250101 --end-date 20250630 --country PE

```
El archivo `log4j2.properties` controla el nivel de logging de Spark y se incluye en el repositorio para garantizar ejecuciones reproducibles y salidas legibles durante el desarrollo

---
## ðŸ—‚ï¸ Estructura 
El proyecto estÃ¡ organizado bajo un enfoque modular para facilitar mantenibilidad, escalabilidad y separaciÃ³n de responsabilidades dentro del flujo ETL. La siguiente estructura refleja los distintos componentes del pipeline, desde la configuraciÃ³n y validaciÃ³n de datos hasta la generaciÃ³n de salidas particionadas.

```text
.
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ run_etl.py
â”œâ”€â”€ log4j2.properties
â”œâ”€â”€ config
â”‚   â””â”€â”€ base.yaml
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ raw
â”‚   â”‚   â””â”€â”€ product_deliveries.csv
â”‚   â””â”€â”€ processed
â”‚       â”œâ”€â”€ pais=PE
â”‚       â”‚   â””â”€â”€ fecha_proceso=20250114
â”‚       â”‚       â””â”€â”€ part-00000.csv
â”‚       â””â”€â”€ pais=GT
â”‚           â””â”€â”€ fecha_proceso=20250513
â”‚               â””â”€â”€ part-00000.csv
â”œâ”€â”€ docs
â”‚   â””â”€â”€ enunciado_prueba.pdf
â”œâ”€â”€ logs
â””â”€â”€ src
    â”œâ”€â”€ main.py
    â”œâ”€â”€ config
    â”œâ”€â”€ filters
    â”‚   â””â”€â”€ selection.py
    â”œâ”€â”€ io
    â”‚   â””â”€â”€ writer.py
    â”œâ”€â”€ transformers
    â”‚   â”œâ”€â”€ units.py
    â”‚   â”œâ”€â”€ deliveries.py
    â”‚   â””â”€â”€ enrichment.py
    â”œâ”€â”€ utils
    â”‚   â”œâ”€â”€ initial_explore.py
    â”‚   â”œâ”€â”€ reporting.py
    â”‚   â””â”€â”€ snake_case.py
    â””â”€â”€ validators
        â””â”€â”€ quality.py

```
## EVALUACIÃ“N

A continuaciÃ³n se describe cÃ³mo cada requerimiento solicitado es abordado dentro del pipeline:

| Requerimiento | ImplementaciÃ³n |
|--------------|----------------|
| Lectura de archivo CSV | Lectura mediante `spark.read.csv()` en `src/main.py` |
| Filtrado por rango de fechas | `filters/selection.py`, utilizando los parÃ¡metros `start-date` y `end-date` definidos vÃ­a `OmegaConf` |
| ParametrizaciÃ³n por paÃ­s | Argumento `--country` en `run_etl.py` |
| Uso de OmegaConf | La configuraciÃ³n del flujo (paths, fechas, paÃ­ses, factores de conversiÃ³n y tipos de entrega) se centraliza en `config/base.yaml` |
| Particionado por fecha | `partitionBy("pais", "fecha_proceso")` en `io/writer.py` |
| NormalizaciÃ³n de unidades (CS â†’ ST) | En `transformers/units.py`, las cantidades expresadas en cajas (CS) se convierten a unidades (ST) utilizando un factor de conversiÃ³n fijo de 1 CS = 20 ST, garantizando homogeneidad de escala y evitando duplicaciÃ³n de registros para anÃ¡lisis separados por tipo de entrega. |
| ClasificaciÃ³n de tipos de entrega | En `transformers/deliveries.py` se filtran Ãºnicamente los valores relevantes de tipo_entrega. ZPRE y ZVE1 se consideran entregas de rutina, mientras que Z04 y Z05 corresponden a bonificaciones. Otros valores se excluyen del output final. Asimismo, se generan las columnas `entrega_rutina_unidades` y `entrega_bonificacion_unidades`, asignando la cantidad normalizada segÃºn corresponda y cero en caso contrario. |
| EstandarizaciÃ³n de nombres de columnas | Se aplica una transformaciÃ³n automÃ¡tica hacia la nomenclatura de `snake_case`en `utils/snake_case.py` inmediatamente despuÃ©s de la lectura del CSV, para asegurar consistencia semÃ¡ntica.|
| DetecciÃ³n y eliminaciÃ³n de anomalÃ­as |  En `utils/initial_explore.py` se genera un perfil del dataset previo a cualquier filtrado, calculando nÃºmero de filas, rango de fechas y en `validators/quality.py` se aplican reglas explÃ­citas de calidad: exclusiÃ³n de registros con `fecha_proceso` o `pais` nulos, precios negativos o nulos, y cantidades menores o iguales a cero.|
| GeneraciÃ³n de mÃ©tricas | En `transformers/enrichment.py` se crea la mÃ©trica `total_value`, calculada como `cantidad_unidades * precio`, permitiendo tener un indicador para un anÃ¡lisis en el futuro.|

### DIAGRAMA DE FLUJO 
```mermaid
flowchart TD

    subgraph INGESTA
        A[(CSV Raw Data)]
        A --> B["spark.read.csv()"]
    end

    subgraph PREPROCESAMIENTO
        B --> C["snake_case(df)<br/>EstandarizaciÃ³n de columnas"]
        C --> D["explore(df)<br/>Perfilado inicial"]
    end

    subgraph CALIDAD_Y_FILTRADO
        D --> E["apply_quality_rules(df)<br/>ValidaciÃ³n de reglas"]
        E --> F["apply_execution_filters(df, cfg)<br/>Rango fechas + PaÃ­s"]
        F --> G["explore(df)<br/>Perfilado post-filtros"]
    end

    subgraph TRANSFORMACIONES
        G --> H["normalize_units(df)<br/>CS â†’ ST"]
        H --> I["classify_deliveries(df)<br/>Rutina vs BonificaciÃ³n"]
        I --> J["enrich_metrics(df)<br/>MÃ©tricas adicionales"]
    end

    subgraph OUTPUT
        J --> K["write_output(df)<br/>CSV particionado<br/>pais / fecha_proceso"]
    end
```

### OUTPUT

El pipeline genera un dataset procesado listo para consumo analÃ­tico, almacenado en formato **CSV** y particionado para facilitar consultas por paÃ­s y fecha de proceso.

```text
data/processed/
â”œâ”€â”€ pais=PE
â”‚   â””â”€â”€ fecha_proceso=YYYYMMDD
â”‚       â””â”€â”€ part-00000.csv
â”œâ”€â”€ pais=GT
â”‚   â””â”€â”€ fecha_proceso=YYYYMMDD
â”‚       â””â”€â”€ part-00000.csv
```text


